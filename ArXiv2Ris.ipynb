{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **First option**"
      ],
      "metadata": {
        "id": "AdIqd5U3_F2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import spacy\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "w5xkYZGhApgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def keyword_extraction(abstract, max_palabras=5):\n",
        "\n",
        "    doc = nlp(abstract)\n",
        "    key_words = [token.text for token in doc if token.pos_ in [\"NOUN\", \"ADJ\"]]\n",
        "    key_words = list(dict.fromkeys(key_words))[:max_palabras]\n",
        "\n",
        "    return key_words"
      ],
      "metadata": {
        "id": "HTAeWisnSGcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_to_ris(dataframe, ris_filename):\n",
        "    with open(ris_filename, mode='w', newline='', encoding='utf-8') as ris_file:\n",
        "\n",
        "        for i,row in dataframe.iterrows():\n",
        "\n",
        "            date_obj = datetime.strptime(str(row['published']), '%Y-%m-%d %H:%M:%S')\n",
        "            year = date_obj.strftime('%Y')\n",
        "            month = date_obj.strftime('%m')\n",
        "            day = date_obj.strftime('%d')\n",
        "\n",
        "            title = row['title'].replace('\\n', ' ').replace('\\r', ' ')\n",
        "\n",
        "            abstract = row['summary'].replace('\\n', ' ').replace('\\r', ' ')\n",
        "\n",
        "            abstract_title = f\"{title}. {abstract}\"\n",
        "            key_words = keyword_extraction(abstract_title,max_palabras=6)\n",
        "\n",
        "            if datetime(2019, 1, 1) <= datetime(int(year), int(month), int(day)) <= datetime(2024, 2, 12):\n",
        "\n",
        "              ris_file.write(\"TY  - \\n\")\n",
        "\n",
        "              ris_file.write(f\"TI  - {title}\\n\")\n",
        "\n",
        "              ris_file.write(f\"SP  - \\n\")\n",
        "\n",
        "              ris_file.write(f\"EP  - \\n\")\n",
        "\n",
        "              for author in row['authors'].split(';'):\n",
        "                ris_file.write(f\"AU  - {author.strip()}\\n\")\n",
        "\n",
        "              ris_file.write(f\"PY  - {year}\\n\")\n",
        "\n",
        "              for word in key_words:\n",
        "                ris_file.write(f\"KW  - {word}\\n\")\n",
        "\n",
        "              ris_file.write(f\"DO  - {row['doi']}\\n\")\n",
        "\n",
        "              ris_file.write(f\"JO  -  \\n\")\n",
        "\n",
        "              ris_file.write(f\"IS  -  \\n\")\n",
        "\n",
        "              ris_file.write(f\"SN  -  \\n\")\n",
        "\n",
        "              ris_file.write(f\"VO  -  \\n\")\n",
        "\n",
        "              ris_file.write(f\"VL  -  \\n\")\n",
        "\n",
        "              ris_file.write(f\"JA  -  \\n\")\n",
        "\n",
        "              ris_file.write(f\"Y1  - {row['published']}\\n\")\n",
        "\n",
        "              ris_file.write(f\"AB  - {abstract}\\n\")\n",
        "\n",
        "              ris_file.write(f\"ER  -  \\n\\n\\n\")\n"
      ],
      "metadata": {
        "id": "yP33L-k7JFQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "import requests\n",
        "\n",
        "\n",
        "def get_arxiv_page(query: str,\n",
        "                   baseURL: str = \"http://export.arxiv.org/api/query?\",\n",
        "                   start: int = 0,\n",
        "                   max_results: int = 7000,\n",
        "                   sortBy: str = \"relevance\",\n",
        "                   sortOrder: str = \"descending\",\n",
        "                   columns: list = [],\n",
        "                   timeout: float = 10.) -> list:\n",
        "    \"\"\"\n",
        "    Function processes the query and returns a list of data rows.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    query : str\n",
        "        Query to be requested\n",
        "    baseURL : str, optional, default : \"http://export.arxiv.org/api/query?\"\n",
        "        Base URL of the arxiv API\n",
        "    start : int, optional, default : 0\n",
        "        Starting index for page\n",
        "    max_results : int, optional, default : 10\n",
        "        Maximum number of page entries\n",
        "    sortBy : str, optional, default : \"relevance\"\n",
        "        Sort entries by\n",
        "    sortOrder : str, optional, default : \"descending\"\n",
        "        Order of sorting\n",
        "    columns : list, optional, default : []\n",
        "        List of columns to be returned\n",
        "    timeout : float, optional, default : 10.\n",
        "        Timeout in seconds for HTTP requests\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    rows : list\n",
        "        List of data rows\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if columns are valid\n",
        "    valid_columns = [\"id\", \"title\", \"summary\", \"authors\", \"primary_category\",\n",
        "                     \"categories\", \"comments\", \"updated\", \"published\", \"doi\", \"links\"]\n",
        "    for c in columns:\n",
        "        if c not in valid_columns:\n",
        "            raise ValueError(\n",
        "                \"{:s} is not a valid column name.\\nValid column names are {}.\".format(c, valid_columns))\n",
        "    # Renaming map of columns for arxiv query\n",
        "    col_map = {\n",
        "        \"id\": \"id\",\n",
        "        \"title\": \"title\",\n",
        "        \"summary\": \"summary\",\n",
        "        \"authors\": \"author\",\n",
        "        \"primary_category\": \"arxiv:primary_category\",\n",
        "        \"categories\": \"category\",\n",
        "        \"comments\": \"arxiv:comment\",\n",
        "        \"updated\": \"updated\",\n",
        "        \"published\": \"published\",\n",
        "        \"doi\": \"arxiv:doi\",\n",
        "        \"links\": \"link\",\n",
        "    }\n",
        "\n",
        "    # Build and request query URL\n",
        "    url = '{:s}{:s}&start={:d}&max_results={:d}&sortBy={:s}&sortOrder={:s}'.format(\n",
        "        baseURL,\n",
        "        query,\n",
        "        start,\n",
        "        max_results,\n",
        "        sortBy,\n",
        "        sortOrder\n",
        "    )\n",
        "\n",
        "    # Retry on server errors or timeouts\n",
        "    retries = requests.adapters.Retry(total=5, backoff_factor=0.5,\n",
        "                                      status_forcelist=[429, 500, 502, 503, 504])\n",
        "    adapter = requests.adapters.HTTPAdapter(max_retries=retries)\n",
        "\n",
        "    http = requests.Session()\n",
        "    http.mount(\"http://\", adapter)\n",
        "    response = http.get(url, timeout=timeout)\n",
        "\n",
        "    # Read data and get entries\n",
        "    data = BeautifulSoup(response.text, \"xml\")\n",
        "    entries = data.find_all(\"entry\")\n",
        "\n",
        "    # Loop over entries and build rows of data frame\n",
        "    rows = []\n",
        "    for entry in entries:\n",
        "\n",
        "        d = {}\n",
        "\n",
        "        # Parse the requested columns\n",
        "        for c in columns:\n",
        "            if c in [\"authors\", \"categories\", \"primary_category\", \"links\"]:\n",
        "                tmp = entry.find_all(col_map[c])\n",
        "                if tmp is not None:\n",
        "                    v = []\n",
        "                    for t in tmp:\n",
        "                        if c == \"authors\":\n",
        "                            v.append(t.find(\"name\").text.strip())\n",
        "                        elif c in [\"categories\", \"primary_category\"]:\n",
        "                            v.append(t[\"term\"].strip())\n",
        "                        elif c == \"links\":\n",
        "                            v.append(t[\"href\"].strip())\n",
        "                    val = \"; \".join(v)\n",
        "            else:\n",
        "                tmp = entry.find(col_map[c])\n",
        "                tmp_stripped = tmp.text.strip() if tmp is not None else \"\"\n",
        "                if c == \"id\":\n",
        "                    val = tmp_stripped[21:]\n",
        "                elif c == \"summary\":\n",
        "                    val = tmp_stripped.replace(\"\\n\", \" \")\n",
        "                elif c in [\"published\", \"updated\"]:\n",
        "                    val = datetime.strptime(tmp_stripped, \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "                else:\n",
        "                    val = tmp_stripped\n",
        "            d[c] = val\n",
        "\n",
        "        rows.append(d)\n",
        "\n",
        "    df = pd.DataFrame(rows, columns=columns)\n",
        "\n",
        "    df.drop_duplicates(inplace=True, ignore_index=True)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "lNqGnEj1Ze4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"search_query=((all:interpre* OR all:expla* OR all:'explainable artificial intelligence') AND (all:'agnostic' OR all:'black-box') AND (all:'method' OR all:'technique') AND ( all:'machine learning' OR all:'deep learning' OR all:'artificial intelligence'))\"\n",
        "df = get_arxiv_page(query,columns = [\"id\", \"title\", \"summary\", \"authors\", \"primary_category\", \"categories\", \"comments\", \"updated\", \"published\", \"doi\", \"links\"])\n",
        "df_to_ris(df, 'ArXiv.ris')"
      ],
      "metadata": {
        "id": "mJrpMAneZsku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwyCgp4VdyFa",
        "outputId": "5e2b3e04-19f2-44e0-8c99-494b8226079f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1332, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Second option**"
      ],
      "metadata": {
        "id": "DCJJ1QsyW_Eq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxftfSM-C4tb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "57b36625-1ad2-4e88-93a2-a6892317208d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-9f3833a03da4>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0metree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mElementTree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mET\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mabout\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minfo\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mglossary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexplain\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/cli/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdebug_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdebug_config\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdebug_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdebug_data\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdebug_diff\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdebug_diff\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdebug_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdebug_model\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/cli/debug_diff.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug_cli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_config_overrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_validation_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0minit_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptimizations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/cli/init_config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mROOT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"templates\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mTEMPLATE_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mROOT\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"quickstart_training.jinja\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mRECOMMENDATIONS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrsly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_yaml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"quickstart_training_recommendations.yml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/srsly/_yaml_api.py\u001b[0m in \u001b[0;36mread_yaml\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforce_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0myaml_loads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/srsly/_yaml_api.py\u001b[0m in \u001b[0;36myaml_loads\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0myaml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomYaml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid YAML: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/srsly/ruamel_yaml/main.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0mconstructor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_constructor_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mconstructor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_single_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/srsly/ruamel_yaml/constructor.py\u001b[0m in \u001b[0;36mget_single_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# type: () -> Any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# Ensure that the stream contains a single document and construct it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomposer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_single_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/srsly/ruamel_yaml/composer.py\u001b[0m in \u001b[0;36mget_single_node\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# type: Any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStreamEndEvent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# Ensure that the stream contains no more documents.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/srsly/ruamel_yaml/composer.py\u001b[0m in \u001b[0;36mcompose_document\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# Compose the root node.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;31m# Drop the DOCUMENT-END event.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/srsly/ruamel_yaml/composer.py\u001b[0m in \u001b[0;36mcompose_node\u001b[0;34m(self, parent, index)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose_sequence_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMappingStartEvent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose_mapping_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascend_resolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/srsly/ruamel_yaml/composer.py\u001b[0m in \u001b[0;36mcompose_mapping_node\u001b[0;34m(self, anchor)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;31m#             start_event.start_mark,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;31m#             \"found duplicate key\", key_event.start_mark)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mitem_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0;31m# node.value[item_key] = item_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/srsly/ruamel_yaml/composer.py\u001b[0m in \u001b[0;36mcompose_node\u001b[0;34m(self, parent, index)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose_sequence_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMappingStartEvent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose_mapping_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascend_resolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/srsly/ruamel_yaml/composer.py\u001b[0m in \u001b[0;36mcompose_mapping_node\u001b[0;34m(self, anchor)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;31m#             start_event.start_mark,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;31m#             \"found duplicate key\", key_event.start_mark)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mitem_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0;31m# node.value[item_key] = item_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/srsly/ruamel_yaml/composer.py\u001b[0m in \u001b[0;36mcompose_node\u001b[0;34m(self, parent, index)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescend_resolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mScalarEvent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose_scalar_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequenceStartEvent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose_sequence_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/srsly/ruamel_yaml/composer.py\u001b[0m in \u001b[0;36mcompose_scalar_node\u001b[0;34m(self, anchor)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mu\"!\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mScalarNode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimplicit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         node = ScalarNode(\n\u001b[1;32m    154\u001b[0m             \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/srsly/ruamel_yaml/resolver.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self, kind, value, implicit)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;31m# type: (Any, Any, Any) -> Any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkind\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mScalarNode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mimplicit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                 \u001b[0mresolvers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversioned_resolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import spacy\n",
        "from datetime import datetime\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def keyword_extraction(abstract, max_palabras=5):\n",
        "\n",
        "    doc = nlp(abstract)\n",
        "    key_words = [token.text for token in doc if token.pos_ in [\"NOUN\", \"ADJ\"]]\n",
        "    key_words = list(dict.fromkeys(key_words))[:max_palabras]\n",
        "\n",
        "    return key_words\n",
        "\n",
        "def guardar_metadatos_arxiv_csv(search_query, max_results=100, filename='arxiv_metadatos.csv'):\n",
        "    url_base = 'http://export.arxiv.org/api/query'\n",
        "    params = {\n",
        "        'search_query': search_query,\n",
        "        'start': 0,\n",
        "        'max_results': max_results,\n",
        "    }\n",
        "\n",
        "    response = requests.get(url_base, params=params)\n",
        "    if response.status_code != 200:\n",
        "        print(\"Error al obtener los datos\")\n",
        "        return\n",
        "\n",
        "    root = ET.fromstring(response.content)\n",
        "\n",
        "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['TY', 'TI', 'AU', 'AB', 'KW', 'Y1', 'UR', 'ID','PY','DO'])\n",
        "\n",
        "        for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
        "\n",
        "            title = entry.find('{http://www.w3.org/2005/Atom}title').text\n",
        "\n",
        "            authors = ', '.join([author.find('{http://www.w3.org/2005/Atom}name').text for author in entry.findall('{http://www.w3.org/2005/Atom}author')])\n",
        "\n",
        "            summary = entry.find('{http://www.w3.org/2005/Atom}summary').text\n",
        "            abstract = f\"{title}. {summary}\"\n",
        "            key_words = keyword_extraction(abstract,max_palabras=6)\n",
        "\n",
        "            published = entry.find('{http://www.w3.org/2005/Atom}published').text\n",
        "\n",
        "            link = entry.find('{http://www.w3.org/2005/Atom}link[@title=\"pdf\"]').attrib['href']\n",
        "\n",
        "            arxiv_id = entry.find('{http://www.w3.org/2005/Atom}id').text\n",
        "\n",
        "            date_obj = datetime.strptime(published, '%Y-%m-%dT%H:%M:%SZ')\n",
        "            year = date_obj.strftime('%Y')\n",
        "\n",
        "            doi = entry.find('{http://www.w3.org/2005/Atom}doi').text if entry.find('{http://www.w3.org/2005/Atom}doi') is not None else ''\n",
        "\n",
        "            writer.writerow(['arXiv', title, authors , summary, ', '.join(key_words), published, link, arxiv_id,year,doi])\n",
        "            #'TY', 'TI', 'AU', 'AB', 'KW', 'Y1', 'UR', 'ID','PY','DO'\n",
        "\n",
        "def csv_to_ris(csv_filename, ris_filename):\n",
        "    with open(csv_filename, mode='r', newline='', encoding='utf-8') as csv_file, \\\n",
        "         open(ris_filename, mode='w', newline='', encoding='utf-8') as ris_file:\n",
        "\n",
        "        csv_reader = csv.DictReader(csv_file)\n",
        "\n",
        "        for row in csv_reader:\n",
        "            ris_file.write(\"TY  - \\n\")\n",
        "\n",
        "            title = row['TI'].replace('\\n', ' ').replace('\\r', ' ')\n",
        "            ris_file.write(f\"TI  - {title}\\n\")\n",
        "\n",
        "            ris_file.write(f\"SP  - \\n\")\n",
        "\n",
        "            ris_file.write(f\"EP  - \\n\")\n",
        "\n",
        "            for author in row['AU'].split(','):\n",
        "              ris_file.write(f\"AU  - {author.strip()}\\n\")\n",
        "\n",
        "            ris_file.write(f\"PY  - {row['PY']}\\n\")\n",
        "\n",
        "            for keyword in row['KW'].split(','):\n",
        "              ris_file.write(f\"KW  - {keyword.strip()}\\n\")\n",
        "\n",
        "            ris_file.write(f\"DO  - {row['DO']}\\n\")\n",
        "\n",
        "            ris_file.write(f\"JO  -  \\n\")\n",
        "\n",
        "            ris_file.write(f\"IS  -  \\n\")\n",
        "\n",
        "            ris_file.write(f\"SN  -  \\n\")\n",
        "\n",
        "            ris_file.write(f\"VO  -  \\n\")\n",
        "\n",
        "            ris_file.write(f\"VL  -  \\n\")\n",
        "\n",
        "            ris_file.write(f\"JA  -  \\n\")\n",
        "\n",
        "            ris_file.write(f\"Y1  - {row['Y1']}\\n\")\n",
        "\n",
        "            abstract = row['AB'].replace('\\n', ' ').replace('\\r', ' ')\n",
        "            ris_file.write(f\"AB  -{abstract[1:-1]}\\n\")\n",
        "\n",
        "            ris_file.write(f\"ER  -  \\n\\n\\n\")\n",
        "\n",
        "\n",
        "search_query = \"((all:interpre* OR all:expla* OR all:'explainable artificial intelligence') AND (all:'agnostic' OR all:'black-box') AND (all:'method' OR all:'technique') AND ( all:'machine learning' OR all:'deep learning' OR all:'artificial intelligence') ) AND (submittedDate:[20190101 TO 20240201])\"\n",
        "file_name_csv = 'ArXiv_Citation_CSV.csv'\n",
        "file_name_ris = 'ArXiv_Citation_RIS.ris'\n",
        "\n",
        "guardar_metadatos_arxiv_csv(search_query, max_results=200, filename= file_name_csv)\n",
        "csv_to_ris(file_name_csv, file_name_ris)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MERGE RIS FILES OF IEEE**\n"
      ],
      "metadata": {
        "id": "fv0GwzrS9hna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def unir_archivos_ris(directorio, archivo_salida):\n",
        "    # Lista para almacenar el contenido de todos los archivos .ris\n",
        "    contenido_total = []\n",
        "\n",
        "    # Recorrer el directorio para encontrar archivos .ris\n",
        "    for archivo in os.listdir(directorio):\n",
        "        if archivo.endswith(\".ris\"):\n",
        "            # Abrir y leer el contenido del archivo .ris\n",
        "            with open(os.path.join(directorio, archivo), 'r', encoding='utf-8') as f:\n",
        "                contenido = f.read()\n",
        "                # Agregar el contenido a la lista total\n",
        "                contenido_total.append(contenido)\n",
        "                # Agregar una línea en blanco entre archivos para separar las entradas\n",
        "                contenido_total.append('\\n')\n",
        "\n",
        "    # Escribir el contenido combinado al archivo de salida\n",
        "    with open(archivo_salida, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(contenido_total))\n",
        "\n",
        "# Usar la función\n",
        "\n",
        "# Usar la función\n",
        "directorio = '/content/MIX'\n",
        "archivo_salida = 'MERGE.ris'\n",
        "unir_archivos_ris(directorio, archivo_salida)\n"
      ],
      "metadata": {
        "id": "XCcmhxnW9r_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contar_ocurrencias_palabra(archivo, palabra):\n",
        "    # Inicializar contador de ocurrencias\n",
        "    contador = 0\n",
        "\n",
        "    # Abrir y leer el archivo\n",
        "    with open(archivo, 'r', encoding='utf-8') as f:\n",
        "        # Leer el archivo línea por línea\n",
        "        for linea in f:\n",
        "            # Contar las ocurrencias de la palabra en la línea actual\n",
        "            contador += linea.count(palabra)\n",
        "\n",
        "    return contador\n",
        "\n",
        "# Ruta al archivo .ris\n",
        "archivo_ris = '/content/MERGE.ris'\n",
        "# Palabra a buscar\n",
        "palabra_a_buscar = 'TY  -'\n",
        "\n",
        "# Contar ocurrencias y mostrar el resultado\n",
        "\n",
        "ocurrencias = contar_ocurrencias_palabra(archivo_ris, palabra_a_buscar)\n",
        "print(f'La palabra \"{palabra_a_buscar}\" se repite {ocurrencias} veces en el archivo.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWCDDjJSAufO",
        "outputId": "7f73aef4-ec04-49cf-db7e-f3e2b4a0a3a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La palabra \"TY  -\" se repite 3347 veces en el archivo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SPRINGER NATURE"
      ],
      "metadata": {
        "id": "Ps-VjC3lRlKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def obtener_datos_springer_formato_pam(api_key, consulta):\n",
        "    url_base = 'http://api.springernature.com/meta/v2/pam'\n",
        "    parametros = {\n",
        "        'q': consulta,\n",
        "        'api_key': api_key,\n",
        "    }\n",
        "\n",
        "    respuesta = requests.get(url_base, params=parametros)\n",
        "\n",
        "    if respuesta.status_code == 200:\n",
        "        # Parsear la respuesta XML\n",
        "        root = ET.fromstring(respuesta.content)\n",
        "\n",
        "        # Lista para almacenar los datos de cada registro\n",
        "        registros = []\n",
        "\n",
        "        # Iterar sobre cada registro en la respuesta XML\n",
        "        for record in root.findall('.//record'):\n",
        "            # Diccionario para almacenar los datos del registro actual\n",
        "            datos_registro = {}\n",
        "            article = record.find('.//pam:message/pam:article', namespaces=root.nsmap)\n",
        "            if article is not None:\n",
        "                for child in article.findall('.//', namespaces=article.nsmap):\n",
        "                    tag = child.tag.split('}')[-1]  # Eliminar el namespace del nombre del tag\n",
        "                    datos_registro[tag] = child.text\n",
        "                # Añadir el diccionario del registro actual a la lista de registros\n",
        "                registros.append(datos_registro)\n",
        "\n",
        "        # Convertir la lista de registros a un DataFrame de Pandas\n",
        "        df = pd.DataFrame(registros)\n",
        "        return df\n",
        "    else:\n",
        "        print(f\"Error en la solicitud: {respuesta.status_code}, Respuesta: {respuesta.text}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "api_key = '426834b47cd5bd8e73a7eb2794eb6fcd'\n",
        "consulta = \"((all:interpre* OR all:expla* OR all:'explainable artificial intelligence') AND (all:'agnostic' OR all:'black-box') AND (all:'method' OR all:'technique') AND (all:'machine learning' OR all:'deep learning' OR all:'artificial intelligence'))\"\n",
        "\n",
        "df_resultado = obtener_datos_springer_formato_pam(api_key, consulta)\n",
        "\n",
        "print(df_resultado)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfCOICU7H6XS",
        "outputId": "5a2acf85-0a3a-498d-ab0a-50c3e2b08184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ACM"
      ],
      "metadata": {
        "id": "qSaaeIfUcYZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Actualizaremos el código para manejar adecuadamente múltiples autores y palabras clave,\n",
        "# y asegurarnos de que el formato de salida cumpla con el orden y estructura solicitados.\n",
        "\n",
        "def process_entry_comprehensive(entry):\n",
        "    # Extrae todos los campos de la entrada\n",
        "    fields = re.findall(r'(\\w{2})  - (.*?)\\n', entry)\n",
        "    field_dict = {}\n",
        "    for field, value in fields:\n",
        "        if field in ['AU', 'KW']:  # Campos que pueden repetirse\n",
        "            if field not in field_dict:\n",
        "                field_dict[field] = [value]\n",
        "            else:\n",
        "                field_dict[field].append(value)\n",
        "        else:\n",
        "            field_dict[field] = value\n",
        "    # Ajusta el tipo de documento y asegura el orden y presencia de campos deseados\n",
        "    # Asumimos que el campo T2 (journal title) y otros campos específicos pueden no estar presentes y los agregamos manualmente si es necesario\n",
        "    field_dict['TY'] = 'JOUR'  # Cambia el tipo de documento a JOUR\n",
        "    if 'T2' not in field_dict:\n",
        "        field_dict['T2'] = ''  # Añade el campo T2 si no existe\n",
        "    # Asegurar todos los campos necesarios\n",
        "    for field in desired_order:\n",
        "        if field not in field_dict:\n",
        "            field_dict[field] = ''\n",
        "    # Ordena y genera la nueva entrada, manejando múltiples autores y palabras clave\n",
        "    new_entry = \"\"\n",
        "    for field in desired_order:\n",
        "        if field in field_dict:\n",
        "            if isinstance(field_dict[field], list):\n",
        "                for item in field_dict[field]:\n",
        "                    new_entry += f\"{field}  - {item}\\n\"\n",
        "            else:\n",
        "                new_entry += f\"{field}  - {field_dict[field]}\\n\"\n",
        "    new_entry += \"ER  -\\n\\n\"\n",
        "    return new_entry\n",
        "\n",
        "# Leer el contenido del archivo RIS actualizado\n",
        "with open(file_path_updated, 'r', encoding='utf-8') as file:\n",
        "    ris_content_updated = file.read()\n",
        "\n",
        "# Encuentra todas las entradas en el contenido\n",
        "entries_updated = entry_pattern.findall(ris_content_updated)\n",
        "\n",
        "# Procesa cada entrada con la lógica actualizada\n",
        "processed_entries_comprehensive = [process_entry_comprehensive(entry) for entry in entries_updated]\n",
        "\n",
        "# Combina todas las entradas procesadas\n",
        "new_ris_content_comprehensive = \"\".join(processed_entries_comprehensive)\n",
        "\n",
        "# Guardar el contenido procesado en un nuevo archivo RIS\n",
        "new_file_path_comprehensive = '/content/ACM_MENDE.ris'\n",
        "with open(new_file_path_comprehensive, 'w', encoding='utf-8') as file:\n",
        "    file.write(new_ris_content_comprehensive)\n",
        "\n",
        "new_file_path_comprehensive\n"
      ],
      "metadata": {
        "id": "PsZdXEaScZ7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Procesar cada entrada con el enfoque mejorado para asegurar la inclusión de todas las citas\n",
        "processed_entries_improved = [process_entry_comprehensive(entry) for entry in entries_improved]\n",
        "\n",
        "# Combina todas las entradas procesadas\n",
        "new_ris_content_improved = \"\".join(processed_entries_improved)\n",
        "\n",
        "# Guardar el contenido procesado en un nuevo archivo RIS, asegurando que todas las entradas estén incluidas\n",
        "new_file_path_improved = '/mnt/data/reorganizado_improved.ris'\n",
        "with open(new_file_path_improved, 'w', encoding='utf-8') as file:\n",
        "    file.write(new_ris_content_improved)\n",
        "\n",
        "new_file_path_improved\n"
      ],
      "metadata": {
        "id": "WyKdXyGbgszX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}